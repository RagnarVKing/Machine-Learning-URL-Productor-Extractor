{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functie care cauta in URL cuvinte cheie care pot contine nume de produs \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def process_good(word):\n",
    "    keywords = ['product', 'item', 'sku', 'brand', 'model', 'name', 'slug', 'tags', 'description', 'code', 'category', 'categ']\n",
    "\n",
    "    if any(keyword in word for keyword in keywords):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pastreaza URL urile care nu contin aceste cuvinte cheie\n",
    "import pandas as pd\n",
    "\n",
    "# Function to check if a word contains only numbers\n",
    "def contains_only_numbers(word):\n",
    "    return word.isdigit()\n",
    "\n",
    "# Function to process the DataFrame and filter out URLs\n",
    "def process_bad(word):\n",
    "    keywords = [\n",
    "        \"about\",\n",
    "        \"contact\",\n",
    "        \"info\",\n",
    "        \"information\",\n",
    "        \"blog\",\n",
    "        \"news\",\n",
    "        \"team\",\n",
    "        \"services\",\n",
    "        \"careers\",\n",
    "        \"faq\",\n",
    "        \"support\",\n",
    "        \"resources\",\n",
    "        \"events\",\n",
    "        \"gallery\",\n",
    "        \"testimonials\",\n",
    "        \"privacy-policy\",\n",
    "        \"terms-of-service\",\n",
    "        \"tos\",\n",
    "        \"press\",\n",
    "        \"partners\",\n",
    "        \"solutions\",\n",
    "        \"case-studies\",\n",
    "        \"portfolio\",\n",
    "        \"our-work\",\n",
    "        \"clients\",\n",
    "        \"awards\",\n",
    "        \"leadership\",\n",
    "        \"mission\",\n",
    "        \"vision\",\n",
    "        \"projects\",\n",
    "        \"subscriptions\",\n",
    "        \"volunteer\",\n",
    "        \"meet-the-team\",\n",
    "        \"pricing\",\n",
    "        \"productSort\",\n",
    "        \"contact-us\"\n",
    "    ]\n",
    "\n",
    "    if any(keyword in word for keyword in keywords) or contains_only_numbers(word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"url_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prelucrate(received_url):\n",
    "    if process_good(received_url):\n",
    "        return 1\n",
    "    if process_bad(received_url):\n",
    "        return 0\n",
    "    \n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement urlparse (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for urlparse\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tokenizer, model, text):\n",
    "\tencoding = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=32)\n",
    "\toutputs = model(**encoding)\n",
    "\tlogits = outputs.logits\n",
    "\tprobs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\tlabel = torch.argmax(probs).item()\n",
    "\treturn label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize_url(url):\n",
    "    tokens = []\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Tokenize the domain\n",
    "    path_tokens = parsed_url.path.split('/')\n",
    "    path_tokens = path_tokens[1:]\n",
    "    tokens.extend(filter(None, path_tokens))\n",
    "    # Tokenize the query string\n",
    "    query_tokens = []\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    for param, values in query_params.items():\n",
    "        query_tokens.append(param)\n",
    "        query_tokens.extend(values)\n",
    "    tokens.extend(query_tokens)\n",
    "    \n",
    "    # Remove empty tokens and return\n",
    "    return ' '.join(filter(None, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/home/vlad/Documents/HACK/HBN/URL-Product-Extraction-ML/checkpoint-150500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model_category_classifier = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "with open('categorii.txt', 'r') as f:\n",
    "    categories = f.readlines()\n",
    "    categories = [cat.strip() for cat in categories]\n",
    "\n",
    "categories = [cat.lower().strip() for cat in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categ = model_category_classifier.encode(categories)\n",
    "def assign_category(word):\n",
    "    w = model_category_classifier.encode(word)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(w, categ)\n",
    "    best_cat_index = cosine_scores.argmax()\n",
    "    best_cat = categories[best_cat_index]\n",
    "\n",
    "    return best_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def extract_urls_from_csv(file_path):\n",
    "    urls = []\n",
    "    i = 0\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            url = row['url']\n",
    "            urls.append(url)\n",
    "            i += 1\n",
    "    return urls\n",
    "\n",
    "file_path = '/Users/dragosmac/_workspace_/hbn/url_product_extraction_input_dataset.csv'  # Replace 'your_file.csv' with the path to your CSV file\n",
    "urls = extract_urls_from_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321245"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    res = prelucrate(url)\n",
    "    if res == 1:\n",
    "        last = url.split('/')[-1]\n",
    "        if last == '':\n",
    "            last = url.split('/')[-2]\n",
    "        with open('good_urls.csv', 'a') as f:\n",
    "            f.write(f'{url}, {last}, {assign_category(last)}\\n')\n",
    "    elif res == 0:\n",
    "        continue\n",
    "    elif res == 2:\n",
    "        res = get_prediction(tokenizer, model, custom_tokenize_url(url))\n",
    "        if res == 1:\n",
    "            last = url.split('/')[-1]\n",
    "            if last == '':\n",
    "                last = url.split('/')[-2]\n",
    "            with open('good_urls.csv', 'a') as f:\n",
    "                f.write(f'{url}, {last}, {assign_category(last)}\\n')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     full_class\u001b[38;5;241m.\u001b[39mappend([url, product])\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgood_url.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 31\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m full_class:\n\u001b[1;32m     34\u001b[0m     category \u001b[38;5;241m=\u001b[39m name[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "urls = urls[0:10000]\n",
    "good_urls = []\n",
    "bad_urls = []\n",
    "still_processing_urls = []\n",
    "for url in urls:\n",
    "    process_res = prelucrate(url)\n",
    "    if process_res == 1:\n",
    "        good_urls.append(url)\n",
    "        continue\n",
    "    if process_res == 0:\n",
    "        bad_urls.append(url)\n",
    "        continue\n",
    "    still_processing_urls.append(url)\n",
    "\n",
    "for url in still_processing_urls:\n",
    "    process_res = get_prediction(tokenizer, model, custom_tokenize_url(url))\n",
    "    if process_res == 1:\n",
    "        good_urls.append(url)\n",
    "        continue\n",
    "    bad_urls.append(url)\n",
    "\n",
    "full_class = []\n",
    "names = []\n",
    "for url in good_urls:\n",
    "    slashes = url.split('/')\n",
    "    product = slashes[:-1]\n",
    "    names.append(product)\n",
    "    full_class.append([url, product])\n",
    "    with open(\"good_url.txt\", \"a\") as file:\n",
    "        file.write(url + \" \" + product + \"\\n\")\n",
    "\n",
    "for name in full_class:\n",
    "    category = name[1]\n",
    "    category = assign_category([name[1]])\n",
    "    name.append(category)\n",
    "\n",
    "for url in good_urls:\n",
    "    with open(\"good_url.txt\", \"a\") as file:\n",
    "        file.write(url + '\\n')\n",
    "\n",
    "for url in bad_urls:\n",
    "    with open(\"bad_url.txt\", \"a\") as file:\n",
    "        file.write(url + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from sentence_transformers) (4.66.2)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.2.2-cp312-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from sentence_transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.13.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading regex-2024.4.16-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.15.1->sentence_transformers)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp312-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.2-cp312-cp312-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.16-cp312-cp312-macosx_11_0_arm64.whl (292 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.5/292.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, urllib3, threadpoolctl, sympy, scipy, safetensors, regex, pyyaml, Pillow, networkx, MarkupSafe, joblib, fsspec, filelock, charset-normalizer, scikit-learn, requests, jinja2, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.3.0 charset-normalizer-3.3.2 filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 jinja2-3.1.3 joblib-1.4.0 mpmath-1.3.0 networkx-3.3 pyyaml-6.0.1 regex-2024.4.16 requests-2.31.0 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 sentence_transformers-2.7.0 sympy-1.12 threadpoolctl-3.4.0 tokenizers-0.19.1 torch-2.2.2 transformers-4.40.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence_transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
